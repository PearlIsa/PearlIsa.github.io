
Open In Colab
import pandas as pd
import seaborn as sns
import sklearn
import matplotlib.pyplot as plt


df = pd.read_csv('/content/drive/MyDrive/Toddler Autism dataset July 2018.csv')
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn import svm
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
from sklearn.preprocessing import RobustScaler
from google.colab import drive
drive.mount('/content/drive')
Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount("/content/drive", force_remount=True).
df.drop('Qchat-10-Score', axis=1, inplace=True)
df.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 1054 entries, 0 to 1053
Data columns (total 18 columns):
 #   Column                  Non-Null Count  Dtype 
---  ------                  --------------  ----- 
 0   Case_No                 1054 non-null   int64 
 1   A1                      1054 non-null   int64 
 2   A2                      1054 non-null   int64 
 3   A3                      1054 non-null   int64 
 4   A4                      1054 non-null   int64 
 5   A5                      1054 non-null   int64 
 6   A6                      1054 non-null   int64 
 7   A7                      1054 non-null   int64 
 8   A8                      1054 non-null   int64 
 9   A9                      1054 non-null   int64 
 10  A10                     1054 non-null   int64 
 11  Age_Mons                1054 non-null   int64 
 12  Sex                     1054 non-null   object
 13  Ethnicity               1054 non-null   object
 14  Jaundice                1054 non-null   object
 15  Family_mem_with_ASD     1054 non-null   object
 16  Who completed the test  1054 non-null   object
 17  Class/ASD Traits        1054 non-null   object
dtypes: int64(12), object(6)
memory usage: 148.3+ KB
df.columns
Index(['Case_No', 'A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10',
       'Age_Mons', 'Sex', 'Ethnicity', 'Jaundice', 'Family_mem_with_ASD',
       'Who completed the test', 'Class/ASD Traits '],
      dtype='object')
from sklearn.preprocessing import LabelEncoder

# create an instance of the LabelEncoder class
le = LabelEncoder()

# encode the 'Ethnicity' column
#df['Ethnicity'] = le.fit_transform(df['Ethnicity'])

# encode the 'Sex' column
df['Sex'] = le.fit_transform(df['Sex'])

# encode the 'jaundice' column
df['Jaundice'] = le.fit_transform(df['Jaundice'])

# encode the 'Family member with ASD history' column
df['Family_mem_with_ASD'] = le.fit_transform(df['Family_mem_with_ASD'])

# encode the 'Who is completing the test' column
#df['Who is completing the test'] = le.fit_transform(df['Who is completing the test'])

# encode the 'Class variable' column
df['Class/ASD Traits '] = le.fit_transform(df['Class/ASD Traits '])
# perform one-hot encoding on the 'Ethnicity' column
ethnicity_dummies = pd.get_dummies(df['Ethnicity'], prefix='Ethnicity')
df = pd.concat([df, ethnicity_dummies], axis=1)

# perform one-hot encoding on the 'Sex' column
sex_dummies = pd.get_dummies(df['Sex'], prefix='Sex')
df = pd.concat([df, sex_dummies], axis=1)

# perform one-hot encoding on the 'jaundice' column
jaundice_dummies = pd.get_dummies(df['Jaundice'], prefix='Jaundice')
df = pd.concat([df, jaundice_dummies], axis=1)

# perform one-hot encoding on the 'Family_mem_with_ASD' column
asd_history_dummies = pd.get_dummies(df['Family_mem_with_ASD'], prefix='Family_mem_with_ASD')
df = pd.concat([df, asd_history_dummies], axis=1)

# perform one-hot encoding on the 'Who completed the test' column
#test_dummies = pd.get_dummies(df['Who completed the test'], prefix='Who completed the test')
#df = pd.concat([df, test_dummies], axis=1)

# perform one-hot encoding on the 'Class/ASD Traits' column
df.columns = df.columns.str.strip()
class_dummies = pd.get_dummies(df['Class/ASD Traits'], prefix='Class/ASD Traits')
df = pd.concat([df, class_dummies], axis=1)
df=df.drop(['Sex', 'Ethnicity', 'Jaundice', 'Family_mem_with_ASD', 'Class/ASD Traits','Who completed the test','Case_No','Who completed the test'], axis=1)
df.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 1054 entries, 0 to 1053
Data columns (total 30 columns):
 #   Column                    Non-Null Count  Dtype
---  ------                    --------------  -----
 0   A1                        1054 non-null   int64
 1   A2                        1054 non-null   int64
 2   A3                        1054 non-null   int64
 3   A4                        1054 non-null   int64
 4   A5                        1054 non-null   int64
 5   A6                        1054 non-null   int64
 6   A7                        1054 non-null   int64
 7   A8                        1054 non-null   int64
 8   A9                        1054 non-null   int64
 9   A10                       1054 non-null   int64
 10  Age_Mons                  1054 non-null   int64
 11  Ethnicity_Hispanic        1054 non-null   uint8
 12  Ethnicity_Latino          1054 non-null   uint8
 13  Ethnicity_Native Indian   1054 non-null   uint8
 14  Ethnicity_Others          1054 non-null   uint8
 15  Ethnicity_Pacifica        1054 non-null   uint8
 16  Ethnicity_White European  1054 non-null   uint8
 17  Ethnicity_asian           1054 non-null   uint8
 18  Ethnicity_black           1054 non-null   uint8
 19  Ethnicity_middle eastern  1054 non-null   uint8
 20  Ethnicity_mixed           1054 non-null   uint8
 21  Ethnicity_south asian     1054 non-null   uint8
 22  Sex_0                     1054 non-null   uint8
 23  Sex_1                     1054 non-null   uint8
 24  Jaundice_0                1054 non-null   uint8
 25  Jaundice_1                1054 non-null   uint8
 26  Family_mem_with_ASD_0     1054 non-null   uint8
 27  Family_mem_with_ASD_1     1054 non-null   uint8
 28  Class/ASD Traits_0        1054 non-null   uint8
 29  Class/ASD Traits_1        1054 non-null   uint8
dtypes: int64(11), uint8(19)
memory usage: 110.3 KB
print(df.columns)
Index(['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons',
       'Ethnicity_Hispanic', 'Ethnicity_Latino', 'Ethnicity_Native Indian',
       'Ethnicity_Others', 'Ethnicity_Pacifica', 'Ethnicity_White European',
       'Ethnicity_asian', 'Ethnicity_black', 'Ethnicity_middle eastern',
       'Ethnicity_mixed', 'Ethnicity_south asian', 'Sex_0', 'Sex_1',
       'Jaundice_0', 'Jaundice_1', 'Family_mem_with_ASD_0',
       'Family_mem_with_ASD_1', 'Class/ASD Traits_0', 'Class/ASD Traits_1'],
      dtype='object')
df.shape
(1054, 30)
df.head
<bound method NDFrame.head of       A1  A2  A3  A4  A5  A6  A7  A8  A9  A10  ...  Ethnicity_mixed  \
0      0   0   0   0   0   0   1   1   0    1  ...                0   
1      1   1   0   0   0   1   1   0   0    0  ...                0   
2      1   0   0   0   0   0   1   1   0    1  ...                0   
3      1   1   1   1   1   1   1   1   1    1  ...                0   
4      1   1   0   1   1   1   1   1   1    1  ...                0   
...   ..  ..  ..  ..  ..  ..  ..  ..  ..  ...  ...              ...   
1049   0   0   0   0   0   0   0   0   0    1  ...                0   
1050   0   0   1   1   1   0   1   0   1    0  ...                0   
1051   1   0   1   1   1   1   1   1   1    1  ...                0   
1052   1   0   0   0   0   0   0   1   0    1  ...                0   
1053   1   1   0   0   1   1   0   1   1    0  ...                0   

      Ethnicity_south asian  Sex_0  Sex_1  Jaundice_0  Jaundice_1  \
0                         0      1      0           0           1   
1                         0      0      1           0           1   
2                         0      0      1           0           1   
3                         0      0      1           1           0   
4                         0      1      0           1           0   
...                     ...    ...    ...         ...         ...   
1049                      0      1      0           1           0   
1050                      0      0      1           0           1   
1051                      0      0      1           0           1   
1052                      0      0      1           1           0   
1053                      0      0      1           0           1   

      Family_mem_with_ASD_0  Family_mem_with_ASD_1  Class/ASD Traits_0  \
0                         1                      0                   1   
1                         1                      0                   0   
2                         1                      0                   0   
3                         1                      0                   0   
4                         0                      1                   0   
...                     ...                    ...                 ...   
1049                      0                      1                   1   
1050                      1                      0                   0   
1051                      1                      0                   0   
1052                      0                      1                   1   
1053                      0                      1                   0   

      Class/ASD Traits_1  
0                      0  
1                      1  
2                      1  
3                      1  
4                      1  
...                  ...  
1049                   0  
1050                   1  
1051                   1  
1052                   0  
1053                   1  

[1054 rows x 30 columns]>
df.isnull().sum()
A1                          0
A2                          0
A3                          0
A4                          0
A5                          0
A6                          0
A7                          0
A8                          0
A9                          0
A10                         0
Age_Mons                    0
Ethnicity_Hispanic          0
Ethnicity_Latino            0
Ethnicity_Native Indian     0
Ethnicity_Others            0
Ethnicity_Pacifica          0
Ethnicity_White European    0
Ethnicity_asian             0
Ethnicity_black             0
Ethnicity_middle eastern    0
Ethnicity_mixed             0
Ethnicity_south asian       0
Sex_0                       0
Sex_1                       0
Jaundice_0                  0
Jaundice_1                  0
Family_mem_with_ASD_0       0
Family_mem_with_ASD_1       0
Class/ASD Traits_0          0
Class/ASD Traits_1          0
dtype: int64
df.describe()
A1	A2	A3	A4	A5	A6	A7	A8	A9	A10	...	Ethnicity_mixed	Ethnicity_south asian	Sex_0	Sex_1	Jaundice_0	Jaundice_1	Family_mem_with_ASD_0	Family_mem_with_ASD_1	Class/ASD Traits_0	Class/ASD Traits_1
count	1054.000000	1054.000000	1054.000000	1054.000000	1054.000000	1054.000000	1054.000000	1054.000000	1054.000000	1054.000000	...	1054.000000	1054.000000	1054.000000	1054.000000	1054.000000	1054.000000	1054.000000	1054.000000	1054.000000	1054.000000
mean	0.563567	0.448767	0.401328	0.512334	0.524668	0.576850	0.649905	0.459203	0.489564	0.586338	...	0.007590	0.056926	0.302657	0.697343	0.726755	0.273245	0.838710	0.161290	0.309298	0.690702
std	0.496178	0.497604	0.490400	0.500085	0.499628	0.494293	0.477226	0.498569	0.500128	0.492723	...	0.086831	0.231811	0.459626	0.459626	0.445837	0.445837	0.367973	0.367973	0.462424	0.462424
min	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	...	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000
25%	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	...	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	1.000000	0.000000	0.000000	0.000000
50%	1.000000	0.000000	0.000000	1.000000	1.000000	1.000000	1.000000	0.000000	0.000000	1.000000	...	0.000000	0.000000	0.000000	1.000000	1.000000	0.000000	1.000000	0.000000	0.000000	1.000000
75%	1.000000	1.000000	1.000000	1.000000	1.000000	1.000000	1.000000	1.000000	1.000000	1.000000	...	0.000000	0.000000	1.000000	1.000000	1.000000	1.000000	1.000000	0.000000	1.000000	1.000000
max	1.000000	1.000000	1.000000	1.000000	1.000000	1.000000	1.000000	1.000000	1.000000	1.000000	...	1.000000	1.000000	1.000000	1.000000	1.000000	1.000000	1.000000	1.000000	1.000000	1.000000
8 rows × 30 columns

df.Age_Mons.describe()
count    1054.000000
mean       27.867173
std         7.980354
min        12.000000
25%        23.000000
50%        30.000000
75%        36.000000
max        36.000000
Name: Age_Mons, dtype: float64
# Create a bar plot
df.mean().plot(kind='bar', figsize=(10,6), color='blue')

# Add title and axis labels
plt.title('Mean values for each variable')
plt.xlabel('Variables')
plt.ylabel('Mean values')

# Show the plot
plt.show()
No description has been provided for this image
for col in df.select_dtypes(include='object').columns:
    print(f"{col}:\n{df[col].value_counts()}\n")
print(df)
      A1  A2  A3  A4  A5  A6  A7  A8  A9  A10  ...  Ethnicity_mixed  \
0      0   0   0   0   0   0   1   1   0    1  ...                0   
1      1   1   0   0   0   1   1   0   0    0  ...                0   
2      1   0   0   0   0   0   1   1   0    1  ...                0   
3      1   1   1   1   1   1   1   1   1    1  ...                0   
4      1   1   0   1   1   1   1   1   1    1  ...                0   
...   ..  ..  ..  ..  ..  ..  ..  ..  ..  ...  ...              ...   
1049   0   0   0   0   0   0   0   0   0    1  ...                0   
1050   0   0   1   1   1   0   1   0   1    0  ...                0   
1051   1   0   1   1   1   1   1   1   1    1  ...                0   
1052   1   0   0   0   0   0   0   1   0    1  ...                0   
1053   1   1   0   0   1   1   0   1   1    0  ...                0   

      Ethnicity_south asian  Sex_0  Sex_1  Jaundice_0  Jaundice_1  \
0                         0      1      0           0           1   
1                         0      0      1           0           1   
2                         0      0      1           0           1   
3                         0      0      1           1           0   
4                         0      1      0           1           0   
...                     ...    ...    ...         ...         ...   
1049                      0      1      0           1           0   
1050                      0      0      1           0           1   
1051                      0      0      1           0           1   
1052                      0      0      1           1           0   
1053                      0      0      1           0           1   

      Family_mem_with_ASD_0  Family_mem_with_ASD_1  Class/ASD Traits_0  \
0                         1                      0                   1   
1                         1                      0                   0   
2                         1                      0                   0   
3                         1                      0                   0   
4                         0                      1                   0   
...                     ...                    ...                 ...   
1049                      0                      1                   1   
1050                      1                      0                   0   
1051                      1                      0                   0   
1052                      0                      1                   1   
1053                      0                      1                   0   

      Class/ASD Traits_1  
0                      0  
1                      1  
2                      1  
3                      1  
4                      1  
...                  ...  
1049                   0  
1050                   1  
1051                   1  
1052                   0  
1053                   1  

[1054 rows x 30 columns]
X_train, X_test, y_train, y_test = train_test_split(df.drop(['Class/ASD Traits_0','Class/ASD Traits_1'], axis=1), df['Class/ASD Traits_1'], test_size=0.2, random_state=50)
classifier = svm.SVC()
hyperparameters = {'kernel': ['linear'],
                   'C': [0.01, 0.1],
                   'gamma': [0.01, 0.1]}
grid_search = GridSearchCV(classifier, hyperparameters, cv=5)
GridSearchCV(cv=5, estimator=svm.SVC(),
             param_grid={'C': [0.01, 0.1], 'gamma': [0.01, 0.1],
                         'kernel': ['linear']})
GridSearchCV
estimator: SVC

SVC
best_classifier = grid_search.estimator
print(best_classifier,'\n')
SVC() 

scaler = RobustScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
grid_search = GridSearchCV(classifier, hyperparameters, cv=5)
grid_search.fit(X_train_scaled, y_train)
GridSearchCV
estimator: SVC

SVC
print("Best hyperparameters:", grid_search.best_params_)
print("Best score:", grid_search.best_score_)
print("Results of all parameter combinations:")
print(grid_search.cv_results_)
Best hyperparameters: {'C': 0.1, 'gamma': 0.01, 'kernel': 'linear'}
Best score: 0.9845660749506904
Results of all parameter combinations:
{'mean_fit_time': array([0.01243119, 0.01178932, 0.00589428, 0.00579367]), 'std_fit_time': array([0.00200517, 0.00341986, 0.00020254, 0.00026791]), 'mean_score_time': array([0.00372157, 0.00337486, 0.00182786, 0.00180774]), 'std_score_time': array([8.91901740e-04, 4.84211345e-04, 3.93584522e-05, 8.28904837e-05]), 'param_C': masked_array(data=[0.01, 0.01, 0.1, 0.1],
             mask=[False, False, False, False],
       fill_value='?',
            dtype=object), 'param_gamma': masked_array(data=[0.01, 0.1, 0.01, 0.1],
             mask=[False, False, False, False],
       fill_value='?',
            dtype=object), 'param_kernel': masked_array(data=['linear', 'linear', 'linear', 'linear'],
             mask=[False, False, False, False],
       fill_value='?',
            dtype=object), 'params': [{'C': 0.01, 'gamma': 0.01, 'kernel': 'linear'}, {'C': 0.01, 'gamma': 0.1, 'kernel': 'linear'}, {'C': 0.1, 'gamma': 0.01, 'kernel': 'linear'}, {'C': 0.1, 'gamma': 0.1, 'kernel': 'linear'}], 'split0_test_score': array([0.9704142 , 0.9704142 , 0.98816568, 0.98816568]), 'split1_test_score': array([0.95266272, 0.95266272, 0.99408284, 0.99408284]), 'split2_test_score': array([0.96449704, 0.96449704, 0.98224852, 0.98224852]), 'split3_test_score': array([0.95833333, 0.95833333, 0.98809524, 0.98809524]), 'split4_test_score': array([0.98809524, 0.98809524, 0.9702381 , 0.9702381 ]), 'mean_test_score': array([0.96680051, 0.96680051, 0.98456607, 0.98456607]), 'std_test_score': array([0.01219344, 0.01219344, 0.00808261, 0.00808261]), 'rank_test_score': array([3, 3, 1, 1], dtype=int32)}
print("Best parameters found:", grid_search.best_params_)
print("Best score:", grid_search.best_score_)
print("CV results:", grid_search.cv_results_)
Best parameters found: {'C': 0.1, 'gamma': 0.01, 'kernel': 'linear'}
Best score: 0.9845660749506904
CV results: {'mean_fit_time': array([0.01243119, 0.01178932, 0.00589428, 0.00579367]), 'std_fit_time': array([0.00200517, 0.00341986, 0.00020254, 0.00026791]), 'mean_score_time': array([0.00372157, 0.00337486, 0.00182786, 0.00180774]), 'std_score_time': array([8.91901740e-04, 4.84211345e-04, 3.93584522e-05, 8.28904837e-05]), 'param_C': masked_array(data=[0.01, 0.01, 0.1, 0.1],
             mask=[False, False, False, False],
       fill_value='?',
            dtype=object), 'param_gamma': masked_array(data=[0.01, 0.1, 0.01, 0.1],
             mask=[False, False, False, False],
       fill_value='?',
            dtype=object), 'param_kernel': masked_array(data=['linear', 'linear', 'linear', 'linear'],
             mask=[False, False, False, False],
       fill_value='?',
            dtype=object), 'params': [{'C': 0.01, 'gamma': 0.01, 'kernel': 'linear'}, {'C': 0.01, 'gamma': 0.1, 'kernel': 'linear'}, {'C': 0.1, 'gamma': 0.01, 'kernel': 'linear'}, {'C': 0.1, 'gamma': 0.1, 'kernel': 'linear'}], 'split0_test_score': array([0.9704142 , 0.9704142 , 0.98816568, 0.98816568]), 'split1_test_score': array([0.95266272, 0.95266272, 0.99408284, 0.99408284]), 'split2_test_score': array([0.96449704, 0.96449704, 0.98224852, 0.98224852]), 'split3_test_score': array([0.95833333, 0.95833333, 0.98809524, 0.98809524]), 'split4_test_score': array([0.98809524, 0.98809524, 0.9702381 , 0.9702381 ]), 'mean_test_score': array([0.96680051, 0.96680051, 0.98456607, 0.98456607]), 'std_test_score': array([0.01219344, 0.01219344, 0.00808261, 0.00808261]), 'rank_test_score': array([3, 3, 1, 1], dtype=int32)}
best_classifier = grid_search.best_estimator_
best_classifier.fit(X_train_scaled, y_train)

y_pred = best_classifier.predict(X_test_scaled)
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
[[ 64   2]
 [  3 142]]
              precision    recall  f1-score   support

           0       0.96      0.97      0.96        66
           1       0.99      0.98      0.98       145

    accuracy                           0.98       211
   macro avg       0.97      0.97      0.97       211
weighted avg       0.98      0.98      0.98       211

sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, cmap="YlGnBu")
<Axes: >
No description has been provided for this image
import numpy as np

# Define data
confusion_matrix = np.array([[64, 2], [3, 142]])
labels = ['True Neg', 'False Pos', 'False Neg', 'True Pos']
values = confusion_matrix.ravel()

# Create bar chart
fig, ax = plt.subplots()
ax.barh(labels, values)

# Set axis labels and title
ax.set_xlabel('Count')
ax.set_ylabel('Confusion Matrix')
ax.set_title('Confusion Matrix for SVM Classifier')

# Show plot
plt.show()
No description has been provided for this image
import matplotlib.pyplot as plt
importances = pd.Series(best_classifier.coef_[0], index=X_train.columns)
importances=abs(importances)
print(importances,'\n')
importances.plot(kind='barh')
plt.show()
A1                          0.805608
A2                          1.064302
A3                          0.492933
A4                          0.906916
A5                          0.865793
A6                          0.937191
A7                          0.951416
A8                          0.950925
A9                          1.239604
A10                         0.684016
Age_Mons                    0.027712
Ethnicity_Hispanic          0.032317
Ethnicity_Latino            0.300000
Ethnicity_Native Indian     0.100000
Ethnicity_Others            0.176250
Ethnicity_Pacifica          0.000000
Ethnicity_White European    0.159302
Ethnicity_asian             0.094800
Ethnicity_black             0.144065
Ethnicity_middle eastern    0.200000
Ethnicity_mixed             0.000000
Ethnicity_south asian       0.200000
Sex_0                       0.138845
Sex_1                       0.138845
Jaundice_0                  0.202830
Jaundice_1                  0.202830
Family_mem_with_ASD_0       0.052226
Family_mem_with_ASD_1       0.052226
dtype: float64 

No description has been provided for this image
# Print the coefficients of the hyperplane
coefficients = best_classifier.coef_
print(coefficients,'\n')
coefficients = abs(coefficients)
print(coefficients,'\n')
# Display the coefficients
#import matplotlib.pyplot as plt
#%matplotlib inline

fig, ax = plt.subplots(figsize=(45, 16))
sns.barplot(x=X_train.columns, y=coefficients.reshape(-1))
ax.set_title("Feature Importance Plot")
plt.show()
[[ 0.80560784  1.06430227  0.49293293  0.90691601  0.86579292  0.93719067
   0.9514163   0.9509252   1.23960436  0.6840156  -0.02771222  0.03231749
   0.3         0.1         0.17625     0.         -0.15930216  0.0947998
  -0.14406512 -0.2         0.         -0.2        -0.13884505  0.13884505
  -0.20282993  0.20282993  0.05222649 -0.05222649]] 

[[0.80560784 1.06430227 0.49293293 0.90691601 0.86579292 0.93719067
  0.9514163  0.9509252  1.23960436 0.6840156  0.02771222 0.03231749
  0.3        0.1        0.17625    0.         0.15930216 0.0947998
  0.14406512 0.2        0.         0.2        0.13884505 0.13884505
  0.20282993 0.20282993 0.05222649 0.05222649]] 

No description has been provided for this image
# Create a box plot for multiple features
plt.boxplot([df['Age_Mons']])
plt.title('Box plot of Age_Mons')
plt.xticks([1], ['Age_Mons'])
plt.show()
No description has been provided for this image
plt.boxplot([df['Age_Mons']], vert=False)
plt.title('Box plot of Age_Mons')
plt.yticks([1], ['Age_Mons'])
plt.show()
No description has been provided for this image
import seaborn as sns

# create a pandas DataFrame with the values
data = {'Class/ASD Traits_0': ['No', 'Yes'], 'Count': [1356, 538]}
df = pd.DataFrame(data)

# create a bar chart
sns.set_style('whitegrid')
sns.barplot(x='Class/ASD Traits_0', y='Count', data=df)
plt.title('Bar Chart of Class/ASD Traits_0')
plt.show()
No description has been provided for this image
# create a pandas DataFrame with the values
data = {'Class/ASD Traits_1': ['No', 'Yes'], 'Count': [1356, 538]}
df = pd.DataFrame(data)

# create a bar chart
sns.set_style('whitegrid')
sns.barplot(x='Class/ASD Traits_1', y='Count', data=df)
plt.title('Bar Chart of Class/ASD Traits_1')
plt.show()
No description has been provided for this image
# create a pandas Series with the values
values = pd.Series([1356, 538], index=['Class/ASD Traits_0', 'Class/ASD Traits_1'])

# create a pie chart
fig, ax = plt.subplots(figsize=(6, 6))
ax.pie(values, labels=values.index, autopct='%1.1f%%', startangle=90)
ax.axis('equal')
ax.set_title('Pie Chart of Class/ASD Traits')
plt.show()
No description has been provided for this image
# Import necessary libraries
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report


# Train the model
rfc = RandomForestClassifier(n_estimators=100)
rfc.fit(X_train, y_train)

# Make predictions on test set
y_pred = rfc.predict(X_test)

# Calculate precision, recall, and F1 score
print(classification_report(y_test, y_pred))

# Fit the random forest model
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Make predictions on the test set
y_pred = rf.predict(X_test)

# Calculate the accuracy score
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy)
              precision    recall  f1-score   support

           0       0.93      0.83      0.88        66
           1       0.93      0.97      0.95       145

    accuracy                           0.93       211
   macro avg       0.93      0.90      0.91       211
weighted avg       0.93      0.93      0.93       211

Accuracy: 0.9146919431279621
# Create a Random Forest Classifier object with desired parameters
rf = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=42)

# Fit the model on the training data
rf.fit(X_train, y_train)

# Access all the decision trees in the fitted Random Forest Classifier
trees = rf.estimators_

# Loop over all the trees and print the feature importances
for i, tree in enumerate(trees):
    print(f'Tree {i} Feature Importances: {tree.feature_importances_}')
Tree 0 Feature Importances: [0.         0.07414126 0.         0.         0.18671575 0.00128107
 0.49384189 0.         0.23866065 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00535937 0.
 0.         0.         0.         0.        ]
Tree 1 Feature Importances: [0.         0.         0.         0.01942578 0.27769142 0.15425376
 0.50753863 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.04109041 0.
 0.         0.         0.         0.        ]
Tree 2 Feature Importances: [2.73847750e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
 5.96988675e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00
 3.64307233e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 2.06826381e-03 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 9.34208269e-05 0.00000000e+00
 0.00000000e+00 9.15763237e-03 0.00000000e+00 0.00000000e+00]
Tree 3 Feature Importances: [0.         0.         0.         0.         0.22055357 0.00895359
 0.00075656 0.         0.73395296 0.00843405 0.02098798 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00636128 0.        ]
Tree 4 Feature Importances: [0.21789834 0.         0.         0.2177039  0.         0.51599443
 0.04569733 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.002706   0.        ]
Tree 5 Feature Importances: [5.83537124e-02 2.98682683e-04 0.00000000e+00 0.00000000e+00
 4.31006290e-01 3.98403962e-02 0.00000000e+00 4.10004504e-01
 4.32518646e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 1.72445509e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00]
Tree 6 Feature Importances: [0.         0.42253014 0.         0.35483816 0.09050139 0.1175429
 0.0047375  0.         0.         0.         0.00984992 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.        ]
Tree 7 Feature Importances: [0.50307217 0.00580732 0.         0.         0.12232204 0.
 0.         0.         0.36215209 0.         0.00664637 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.        ]
Tree 8 Feature Importances: [0.         0.02564045 0.         0.55078968 0.27507906 0.0915144
 0.05697641 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.        ]
Tree 9 Feature Importances: [0.         0.         0.         0.03405919 0.         0.25317409
 0.20055562 0.         0.43668594 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.07552516 0.         0.         0.         0.
 0.         0.         0.         0.        ]
Tree 10 Feature Importances: [0.00130729 0.         0.32914088 0.         0.         0.
 0.         0.13666243 0.46925521 0.05428037 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00935381 0.
 0.         0.         0.         0.        ]
Tree 11 Feature Importances: [0.02254919 0.05353693 0.         0.05091208 0.56630083 0.27189767
 0.         0.         0.00157223 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.03323107 0.         0.         0.         0.
 0.         0.         0.         0.        ]
Tree 12 Feature Importances: [3.13370807e-02 0.00000000e+00 0.00000000e+00 4.77281879e-02
 4.94927315e-01 0.00000000e+00 3.25697563e-03 8.43330293e-02
 3.38338679e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 7.87327755e-05 0.00000000e+00 0.00000000e+00 0.00000000e+00]
Tree 13 Feature Importances: [0.00469124 0.         0.         0.24799041 0.         0.
 0.66124887 0.         0.07095586 0.         0.         0.
 0.         0.         0.         0.         0.00467262 0.010441
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.        ]
Tree 14 Feature Importances: [0.         0.         0.         0.         0.10920229 0.65094164
 0.11882686 0.         0.02879137 0.         0.         0.
 0.         0.         0.         0.         0.         0.00960277
 0.         0.00741847 0.         0.         0.07521659 0.
 0.         0.         0.         0.        ]
Tree 15 Feature Importances: [0.01572564 0.         0.35188275 0.10288912 0.04089008 0.4879086
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.00070382 0.
 0.         0.         0.         0.        ]
Tree 16 Feature Importances: [0.         0.         0.03702623 0.         0.0236338  0.23841143
 0.         0.31366789 0.38675756 0.         0.0005031  0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.        ]
Tree 17 Feature Importances: [0.33420469 0.         0.         0.41701806 0.11843208 0.05391226
 0.05916212 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.0011047  0.
 0.         0.         0.         0.         0.         0.
 0.         0.0161661  0.         0.        ]
Tree 18 Feature Importances: [0.51522777 0.         0.         0.35388577 0.         0.04729347
 0.04669304 0.         0.         0.         0.         0.
 0.01902324 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.0064919  0.
 0.         0.         0.         0.01138482]
Tree 19 Feature Importances: [3.12325997e-01 0.00000000e+00 2.62382044e-04 0.00000000e+00
 0.00000000e+00 1.31007781e-02 2.67021995e-02 0.00000000e+00
 6.23052206e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 2.45564372e-02 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]
Tree 20 Feature Importances: [0.         0.         0.006215   0.61924425 0.         0.
 0.00491327 0.00635974 0.35769965 0.         0.00429493 0.
 0.         0.         0.         0.         0.00127314 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.        ]
Tree 21 Feature Importances: [0.         0.         0.         0.53102767 0.0316983  0.34121016
 0.07302443 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.02025851 0.         0.         0.         0.
 0.         0.00278093 0.         0.        ]
Tree 22 Feature Importances: [0.00000000e+00 8.75239946e-02 0.00000000e+00 2.02641998e-01
 3.20406191e-02 0.00000000e+00 5.47397588e-01 0.00000000e+00
 1.15205839e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 2.45537996e-04 0.00000000e+00
 1.49444223e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00]
Tree 23 Feature Importances: [0.04933721 0.         0.02042688 0.49915398 0.10986098 0.00659426
 0.         0.         0.29974514 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.01488156
 0.         0.         0.         0.        ]
Tree 24 Feature Importances: [4.53680955e-01 2.26006008e-02 0.00000000e+00 0.00000000e+00
 8.62380670e-02 3.52799904e-01 0.00000000e+00 8.44527219e-02
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 2.27751476e-04 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]
Tree 25 Feature Importances: [0.         0.         0.         0.24618315 0.         0.
 0.         0.02200271 0.66454045 0.         0.0672737  0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.        ]
Tree 26 Feature Importances: [0.         0.08325816 0.01476448 0.24596145 0.         0.35797466
 0.         0.         0.20151253 0.         0.0258648  0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.07066392
 0.         0.         0.         0.        ]
Tree 27 Feature Importances: [0.04260518 0.         0.         0.         0.0067058  0.06998286
 0.67115336 0.19942607 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01012674 0.         0.         0.         0.         0.
 0.         0.         0.         0.        ]
Tree 28 Feature Importances: [0.40076302 0.06701578 0.         0.         0.         0.01226074
 0.         0.14528814 0.35447317 0.         0.         0.
 0.         0.         0.         0.         0.         0.00171049
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01848867 0.        ]
Tree 29 Feature Importances: [0.10393894 0.19869998 0.         0.3230347  0.         0.
 0.         0.37432638 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.        ]
Tree 30 Feature Importances: [0.         0.         0.         0.         0.55075669 0.26628594
 0.08699058 0.07747612 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.01135745 0.         0.         0.00713322
 0.         0.         0.         0.        ]
Tree 31 Feature Importances: [0.         0.         0.         0.         0.00077171 0.185108
 0.57705113 0.         0.18215917 0.         0.02088693 0.
 0.         0.         0.         0.         0.01628992 0.
 0.         0.         0.         0.01773313 0.         0.
 0.         0.         0.         0.        ]
Tree 32 Feature Importances: [0.         0.         0.         0.         0.         0.00292908
 0.32767573 0.09106586 0.5300814  0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.02511554 0.         0.
 0.         0.         0.02313238 0.        ]
Tree 33 Feature Importances: [0.         0.         0.         0.         0.23864396 0.0193433
 0.         0.17578676 0.44871348 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.09512972 0.         0.         0.         0.
 0.         0.         0.         0.02238278]
Tree 34 Feature Importances: [0.30004024 0.01345627 0.         0.         0.51478301 0.
 0.0362485  0.         0.         0.         0.07388326 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.06158872 0.         0.         0.        ]
Tree 35 Feature Importances: [0.00000000e+00 1.57110919e-01 0.00000000e+00 0.00000000e+00
 2.12640406e-01 6.04108340e-02 0.00000000e+00 0.00000000e+00
 5.68138649e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 5.04133995e-04 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 9.14675926e-04 0.00000000e+00
 0.00000000e+00 2.80382285e-04 0.00000000e+00 0.00000000e+00]
Tree 36 Feature Importances: [0.00000000e+00 2.98457636e-02 0.00000000e+00 4.70722205e-01
 4.22574040e-02 3.86840359e-01 3.46773070e-02 0.00000000e+00
 1.61676254e-04 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 3.54952852e-02 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]
Tree 37 Feature Importances: [0.22553726 0.04436507 0.06840297 0.01032385 0.         0.
 0.         0.         0.63203179 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01826424 0.
 0.         0.00107482 0.         0.        ]
Tree 38 Feature Importances: [0.         0.00197636 0.05143195 0.         0.20426091 0.10249838
 0.54587744 0.04374402 0.05021094 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.        ]
Tree 39 Feature Importances: [0.         0.03443966 0.         0.         0.12477276 0.60704706
 0.2202041  0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01353641 0.         0.         0.         0.         0.
 0.         0.         0.         0.        ]
Tree 40 Feature Importances: [0.02544236 0.12718131 0.         0.         0.23722802 0.55600322
 0.05270876 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00143633 0.         0.        ]
Tree 41 Feature Importances: [0.57357002 0.         0.01337319 0.         0.10390048 0.
 0.05253752 0.25126645 0.         0.         0.         0.
 0.         0.         0.00535234 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.        ]
Tree 42 Feature Importances: [0.         0.03595786 0.09331632 0.         0.         0.58323763
 0.28525664 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00223156 0.        ]
Tree 43 Feature Importances: [0.02538954 0.         0.         0.03636389 0.08408037 0.58182441
 0.27081395 0.00152785 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.        ]
Tree 44 Feature Importances: [0.49857465 0.         0.         0.25789816 0.14907299 0.0372964
 0.         0.         0.         0.02875533 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.02840247 0.         0.         0.         0.
 0.         0.         0.         0.        ]
Tree 45 Feature Importances: [4.04251205e-01 1.30678558e-01 3.20368630e-04 0.00000000e+00
 9.71478181e-02 0.00000000e+00 2.82528081e-01 0.00000000e+00
 6.53640757e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.97098930e-02
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]
Tree 46 Feature Importances: [0.         0.26043538 0.         0.13411149 0.37192498 0.
 0.16770803 0.         0.0130309  0.05278923 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.        ]
Tree 47 Feature Importances: [0.00000000e+00 3.36955882e-04 1.96586835e-02 0.00000000e+00
 5.41829347e-02 1.01320610e-02 3.07506280e-01 0.00000000e+00
 5.92865489e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.53175959e-02
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]
Tree 48 Feature Importances: [0.0055962  0.         0.         0.         0.07769932 0.17650334
 0.48241839 0.         0.25778275 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.        ]
Tree 49 Feature Importances: [0.         0.0297429  0.         0.         0.03522286 0.52501698
 0.         0.00385734 0.40615992 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.        ]
Tree 50 Feature Importances: [0.24046503 0.         0.         0.         0.03210051 0.
 0.         0.04363353 0.63105049 0.         0.05275044 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.        ]
Tree 51 Feature Importances: [0.46983584 0.0299394  0.         0.38275768 0.04911427 0.
 0.0503493  0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.01800351 0.         0.        ]
Tree 52 Feature Importances: [0.53761324 0.         0.02113811 0.         0.43108524 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.0101634  0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.        ]
Tree 53 Feature Importances: [0.00000000e+00 0.00000000e+00 0.00000000e+00 5.49969341e-01
 3.09354070e-02 3.24876264e-02 2.07223418e-04 5.67028153e-02
 3.18793122e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.09044653e-02
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]
Tree 54 Feature Importances: [5.50109209e-02 0.00000000e+00 0.00000000e+00 1.61489649e-01
 4.36365801e-01 1.47700842e-01 4.72137599e-02 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.51842189e-01
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 3.76839250e-04 0.00000000e+00 0.00000000e+00 0.00000000e+00]
Tree 55 Feature Importances: [0.         0.         0.         0.01899282 0.         0.00575543
 0.56227567 0.06749662 0.29652215 0.         0.01764631 0.
 0.         0.         0.         0.         0.         0.03131101
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.        ]
Tree 56 Feature Importances: [0.         0.         0.         0.         0.04608996 0.
 0.58457503 0.20497317 0.1373306  0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00582307 0.         0.02120817 0.         0.
 0.         0.         0.         0.        ]
Tree 57 Feature Importances: [0.42049906 0.34441074 0.         0.22230109 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01278911 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.        ]
Tree 58 Feature Importances: [0.00448915 0.09291746 0.15047336 0.         0.66262565 0.
 0.         0.         0.         0.         0.08819904 0.00129533
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.        ]
Tree 59 Feature Importances: [0.00000000e+00 1.78102930e-01 0.00000000e+00 2.04646468e-01
 4.96084217e-01 0.00000000e+00 4.18656086e-02 0.00000000e+00
 4.68521838e-02 3.23941262e-02 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.44660831e-05]
Tree 60 Feature Importances: [0.         0.         0.07177798 0.         0.06853204 0.
 0.56472158 0.00592363 0.27980324 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00924153 0.         0.
 0.         0.         0.         0.        ]
Tree 61 Feature Importances: [0.06108315 0.01842749 0.         0.32670094 0.         0.13180438
 0.         0.38904199 0.07294205 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.        ]
Tree 62 Feature Importances: [0.15247391 0.         0.         0.         0.53308944 0.29127218
 0.         0.         0.         0.         0.01230099 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.01086348 0.         0.         0.        ]
Tree 63 Feature Importances: [0.00000000e+00 7.55200031e-02 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 7.56245709e-01 4.00516661e-02
 1.06149024e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 2.19825413e-02 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 5.10565366e-05 0.00000000e+00 0.00000000e+00 0.00000000e+00]
Tree 64 Feature Importances: [0.         0.02447747 0.03173732 0.00094913 0.27496695 0.59603705
 0.         0.07183208 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.        ]
Tree 65 Feature Importances: [0.10706592 0.         0.         0.05789023 0.         0.11182367
 0.61765385 0.         0.08303245 0.         0.02253388 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.        ]
Tree 66 Feature Importances: [0.02990683 0.         0.         0.08535339 0.07260791 0.67574255
 0.         0.11307065 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00910497 0.0142137  0.        ]
Tree 67 Feature Importances: [1.00804076e-04 0.00000000e+00 0.00000000e+00 3.73196505e-02
 0.00000000e+00 6.15771296e-01 2.21683533e-01 4.31495845e-02
 7.51110288e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 6.86410313e-03 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]
Tree 68 Feature Importances: [0.07906706 0.         0.         0.33474941 0.         0.13219705
 0.         0.38176198 0.         0.0069583  0.0652662  0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.        ]
Tree 69 Feature Importances: [0.         0.         0.         0.         0.1440818  0.23497896
 0.59040015 0.         0.         0.         0.01390816 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.00887584
 0.00775509 0.         0.         0.        ]
Tree 70 Feature Importances: [0.00000000e+00 0.00000000e+00 0.00000000e+00 7.81860752e-02
 1.40674403e-02 2.74221795e-01 0.00000000e+00 0.00000000e+00
 5.84586648e-01 4.83728987e-02 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 5.65142424e-04 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]
Tree 71 Feature Importances: [0.4552684  0.         0.01153536 0.04123497 0.         0.39019166
 0.07554595 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.02622365
 0.         0.         0.         0.        ]
Tree 72 Feature Importances: [0.19585295 0.         0.         0.00441025 0.25390575 0.
 0.14705554 0.         0.37842601 0.02034951 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.        ]
Tree 73 Feature Importances: [0.08575432 0.         0.11409149 0.         0.01049056 0.24850942
 0.54115421 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.        ]
Tree 74 Feature Importances: [0.         0.         0.         0.         0.075883   0.
 0.6357606  0.16584899 0.05976718 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.05460328 0.00813695 0.
 0.         0.         0.         0.        ]
Tree 75 Feature Importances: [0.11528055 0.         0.         0.         0.         0.29509922
 0.         0.         0.50216271 0.         0.00203655 0.
 0.         0.         0.         0.         0.         0.
 0.         0.08542096 0.         0.         0.         0.
 0.         0.         0.         0.        ]
Tree 76 Feature Importances: [0.23568865 0.         0.         0.00869107 0.         0.60279076
 0.04189957 0.         0.05076244 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.06016751
 0.         0.         0.         0.        ]
Tree 77 Feature Importances: [0.17236621 0.         0.         0.         0.         0.35626789
 0.         0.31598595 0.15537995 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.        ]
Tree 78 Feature Importances: [2.39920964e-01 0.00000000e+00 0.00000000e+00 7.43381718e-02
 0.00000000e+00 2.31159630e-02 0.00000000e+00 6.26072673e-02
 5.99917191e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 1.00443272e-04 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]
Tree 79 Feature Importances: [0.00878224 0.         0.         0.10269695 0.         0.
 0.10081439 0.         0.76038193 0.         0.         0.
 0.         0.         0.         0.         0.01107674 0.
 0.         0.         0.         0.         0.         0.
 0.01624774 0.         0.         0.        ]
Tree 80 Feature Importances: [0.         0.1261884  0.         0.01834971 0.45017905 0.
 0.         0.         0.28381279 0.         0.01213253 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.07861647
 0.03072105 0.         0.         0.        ]
Tree 81 Feature Importances: [0.         0.         0.02411639 0.6493146  0.         0.
 0.         0.         0.08816433 0.01619986 0.21108964 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.00440383 0.         0.
 0.         0.         0.         0.00671135]
Tree 82 Feature Importances: [0.02290724 0.         0.30917186 0.         0.59686325 0.00099842
 0.         0.         0.06107763 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.0089816
 0.         0.         0.         0.        ]
Tree 83 Feature Importances: [0.22073053 0.         0.         0.         0.55927969 0.06678273
 0.00067854 0.         0.05913789 0.08583348 0.         0.
 0.00755715 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.        ]
Tree 84 Feature Importances: [0.30983067 0.         0.         0.29011618 0.00408936 0.05514746
 0.05064316 0.29017318 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.        ]
Tree 85 Feature Importances: [0.         0.         0.         0.         0.14319557 0.23015264
 0.20199992 0.18040904 0.10023796 0.09756865 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.04643622 0.         0.         0.         0.
 0.         0.         0.         0.        ]
Tree 86 Feature Importances: [0.01127661 0.         0.0775208  0.         0.05535619 0.64140184
 0.182953   0.         0.         0.         0.02290059 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.00859097 0.         0.         0.
 0.         0.         0.         0.        ]
Tree 87 Feature Importances: [0.         0.         0.         0.27831324 0.         0.26021051
 0.02936737 0.         0.31059463 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.05924464 0.         0.         0.06176149 0.
 0.00050811 0.         0.         0.        ]
Tree 88 Feature Importances: [0.02504557 0.17270409 0.         0.         0.02440208 0.56142518
 0.03436098 0.         0.18206211 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.        ]
Tree 89 Feature Importances: [3.12679481e-01 5.59498054e-02 0.00000000e+00 0.00000000e+00
 0.00000000e+00 5.93663568e-01 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 2.29401389e-03
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 1.92943595e-02 0.00000000e+00 0.00000000e+00 9.56198953e-05
 0.00000000e+00 1.60231525e-02 0.00000000e+00 0.00000000e+00]
Tree 90 Feature Importances: [0.         0.         0.         0.         0.         0.59716807
 0.         0.16667349 0.23615844 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.        ]
Tree 91 Feature Importances: [0.25086557 0.         0.         0.         0.         0.581104
 0.13759414 0.01516015 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.00628873 0.         0.00898741]
Tree 92 Feature Importances: [0.00829763 0.15656915 0.2719438  0.00857787 0.         0.48759291
 0.06701863 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.        ]
Tree 93 Feature Importances: [0.         0.         0.02474433 0.         0.         0.67493335
 0.         0.         0.21318234 0.06495485 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.02218512 0.         0.
 0.         0.         0.         0.        ]
Tree 94 Feature Importances: [0.08196989 0.         0.         0.         0.         0.10416266
 0.52725194 0.         0.28579136 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00082415 0.         0.         0.        ]
Tree 95 Feature Importances: [0.         0.         0.         0.         0.10592655 0.06065624
 0.49693044 0.33102068 0.         0.         0.0054661  0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.        ]
Tree 96 Feature Importances: [0.         0.         0.         0.         0.14932153 0.41687773
 0.28772607 0.0672579  0.00948114 0.06933563 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.        ]
Tree 97 Feature Importances: [0.04232873 0.06459511 0.         0.         0.1284318  0.19961044
 0.50339774 0.06101325 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00062293 0.         0.         0.        ]
Tree 98 Feature Importances: [0.08694256 0.         0.         0.38021709 0.         0.06445572
 0.32272315 0.1292638  0.         0.0152545  0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.00114318 0.         0.         0.        ]
Tree 99 Feature Importances: [0.03184994 0.00157579 0.05963316 0.         0.         0.06185582
 0.38609509 0.34991019 0.10908001 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.        ]
from sklearn.tree import export_graphviz
import pydotplus
from IPython.display import Image

# Select a representative tree from the random forest
tree = rf.estimators_[0]

# Export the tree to a Graphviz dot file
dot_data = export_graphviz(tree, out_file=None,
                           feature_names=X_train.columns,
                           filled=True, rounded=True,
                           special_characters=True)
graph = pydotplus.graph_from_dot_data(dot_data)

# Save the tree visualization to a PDF file
graph.write_pdf("decision_tree.pdf")

# Display the tree visualization in Jupyter notebook
Image(graph.create_png())
No description has been provided for this image
# Create a Random Forest Classifier object with desired parameters
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=42)

# Fit the model on the training data
rf.fit(X_train, y_train)

# Get feature importances from the trained model
importances = rf.feature_importances_

# Get feature names from the original dataset
feature_names = X_train.columns

# Create a dictionary of feature names and importances
feature_importances = dict(zip(feature_names, importances))

# Sort the feature importances in descending order
sorted_feature_importances = dict(sorted(feature_importances.items(), key=lambda x: x[1], reverse=True))

# Plot the feature importances
plt.barh(range(len(sorted_feature_importances)), list(sorted_feature_importances.values()), align='center')
plt.yticks(range(len(sorted_feature_importances)), list(sorted_feature_importances.keys()))
plt.xlabel('Feature Importance')
plt.ylabel('Feature')
plt.title('Random Forest Feature Importances')
plt.show()
No description has been provided for this image
